{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6vnZnI57N0t6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !tar -xf /content/drive/My\\ Drive/conda_colab.tar.gz -C ../\n",
        "\n",
        "# import os\n",
        "# path = '/content/miniconda3/bin:' + os.environ['PATH']\n",
        "# %env PATH=$path\n",
        "# %env PYTHONPATH=\n",
        "# import sys\n",
        "# _ = sys.path.append(\"/content/miniconda3/lib/python3.7/site-packages\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ovoZOpofQV",
        "outputId": "bb7103d8-c710-4f2d-9287-88892deba2e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uq7mMKdJgOuA"
      },
      "outputs": [],
      "source": [
        "# This is supposed to make sure if you edit .py files\n",
        "# colab will pick up on it\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c286zxotAGMY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Local\n",
        "if os.path.isdir('PIGEON/'):\n",
        "    PIGEON_DIR = 'PIGEON/'\n",
        "# Git clone when colab\n",
        "else:\n",
        "    PIGEON_DIR = 'PROJECT/PIGEON/'\n",
        "\n",
        "os.environ[\"PIGEON_DIR\"] = PIGEON_DIR\n",
        "sys.path.insert(1, PIGEON_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iS51rJiCGFvj"
      },
      "outputs": [],
      "source": [
        "!if [[ ! -d PROJECT ]] && [[ ! -d PIGEON ]]; then git clone https://github.com/iggy2k/CSC490-Project-Winter-2025.git PROJECT; fi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "from tqdm.notebook import tqdm\n",
        "import subprocess\n",
        "\n",
        "\n",
        "\n",
        "with open(\"./PROJECT/environment.yaml\") as file_handle:\n",
        "    environment_data = yaml.safe_load(file_handle)\n",
        "\n",
        "for dependency in tqdm(environment_data[\"dependencies\"], total=len(environment_data[\"dependencies\"])):\n",
        "    if isinstance(dependency, dict):\n",
        "      for lib in tqdm(dependency['pip'], total=len(dependency['pip'])):\n",
        "        try:\n",
        "            out = subprocess.check_output(f'pip install {lib.split(\"=\")[0]}', shell=True)\n",
        "        except subprocess.CalledProcessError as err:\n",
        "            print(err)\n",
        "    else:\n",
        "      try:\n",
        "          subprocess.check_output(f'pip install {dependency.split(\"=\")[0]}', shell=True)\n",
        "      except subprocess.CalledProcessError as err:\n",
        "          print(err)\n",
        "try:\n",
        "  subprocess.check_output('pip install pycountry global_land_mask', shell=True)\n",
        "except subprocess.CalledProcessError as err:\n",
        "  print(err)"
      ],
      "metadata": {
        "id": "LasBUmIqp1po",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "de640296b9d645a29f9e83a202d84631",
            "7004ff49524240a6b97fdd2475a232ef",
            "14c0d34e54144ad0a3e96629d7f205b7",
            "bf9b0bb780a248a3aa9399a76d8afc5e",
            "6251678cf3444ae292c655cb494ae2aa",
            "2488262443454f5c86fbe9405d673374",
            "fa9c2cd11bea4c6fb5fd29c4dea7035b",
            "da268ea87afd49139edbbde0d182428f",
            "b848f875d6484cbc892906fdde96a5e7",
            "adc4cdf622c941d6a550988a73d9816e",
            "e0c14be38d2e4e67b5f0fb21f4c80674"
          ]
        },
        "outputId": "74d3e83a-5164-40cb-9785-fba3416f26bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/26 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de640296b9d645a29f9e83a202d84631"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Command 'pip install matplotlib-base' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz4Nm0VPNvir"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !tar -zcf conda_colab.tar.gz /content/miniconda3\n",
        "# !cp conda_colab.tar.gz /content/drive/My\\ Drive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BpUyGTFsmpg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ILwLpBe2LK"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('datasets/osv5m/images'):\n",
        "    snapshot_download(repo_id=\"osv5m/osv5m\", local_dir=\"datasets/osv5m\", allow_patterns=[\n",
        "        'images/train/00.zip',\n",
        "        'images/test/00.zip',\n",
        "        '*.csv'\n",
        "        ], repo_type='dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JxsgC4Ne7Tj"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from time import sleep\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.notebook import trange, tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sk_1i7If3mt"
      },
      "outputs": [],
      "source": [
        "for root, dirs, files in os.walk(\"datasets/osv5m\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".zip\"):\n",
        "            with zipfile.ZipFile(os.path.join(root, file), 'r') as zip_ref:\n",
        "                for member in tqdm(zip_ref.infolist(), desc=f'Extracting {os.path.join(root, file)}'):\n",
        "                  zip_ref.extract(member, root)\n",
        "            os.remove(os.path.join(root, file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG-kxcaQfsgj"
      },
      "outputs": [],
      "source": [
        "import pycountry\n",
        "\n",
        "# Debug\n",
        "MAX_ITEMS = 1000\n",
        "\n",
        "class ImageCoordinateDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dirs, transform=None):\n",
        "        self.data = []\n",
        "        self.files = []\n",
        "        self.images = []\n",
        "        self.skipped = 0\n",
        "        self.csv = csv_file\n",
        "        self.transform = transform\n",
        "\n",
        "        for image_dir in image_dirs:\n",
        "          print('Reading', image_dir)\n",
        "\n",
        "          self.files.extend([f\"{image_dir}/{f}\" for f in listdir(image_dir) if isfile(join(image_dir, f))])\n",
        "\n",
        "          print(f'Found {len(self.files)} files.')\n",
        "\n",
        "          if not os.path.isdir('datasets/osv5m/'):\n",
        "            os.makedirs('datasets/osv5m/')\n",
        "          if isfile(f\"{csv_file}_filtered.csv\"):\n",
        "            self.df = pd.read_csv(f\"{csv_file}_filtered.csv\", index_col=False)\n",
        "          else:\n",
        "            self.df = pd.concat([chunk for chunk in tqdm(pd.read_csv(self.csv, chunksize=5000, usecols=['id', 'latitude', 'longitude', 'country'], index_col=False), desc='Loading data')])\n",
        "\n",
        "          print(f'Found {len(self.df)} csv entries.')\n",
        "\n",
        "          self.df['country'] = self.df['country'].apply(lambda x: pycountry.countries.get(alpha_2=x).name if pycountry.countries.get(alpha_2=x) else x)\n",
        "\n",
        "          new = pd.DataFrame(columns=['id', 'latitude', 'longitude', 'country'])\n",
        "          i = 0\n",
        "          for full_path in tqdm(self.files, total=len(self.files), desc='Processing files'):\n",
        "              image_name = str(Path(full_path).stem)\n",
        "\n",
        "              try:\n",
        "                row = self.df[self.df['id'] == int(image_name)].iloc[0]\n",
        "              except:\n",
        "                continue\n",
        "              new.loc[i] = row\n",
        "              lat = row['latitude']\n",
        "              lon = row['longitude']\n",
        "\n",
        "              # Remove mislaballed images (ocean pictures?)\n",
        "              # if not globe.is_land(float(lat), float(lon)):\n",
        "              #   self.skipped += 1\n",
        "              #   continue\n",
        "              self.data.append(np.array([str(full_path), float(lat), float(lon)]))\n",
        "              i += 1\n",
        "\n",
        "          self.df = new\n",
        "        if MAX_ITEMS is not None:\n",
        "          print(f'Keeping {MAX_ITEMS} items')\n",
        "          self.df = self.df[:MAX_ITEMS]\n",
        "          self.data = self.data[:MAX_ITEMS]\n",
        "          self.files = self.files[:MAX_ITEMS]\n",
        "\n",
        "        self.df.to_csv(f\"{csv_file}_filtered.csv\", index=False)\n",
        "\n",
        "        print(f'Dataset ready, {len(self.files)} files.')\n",
        "        print(f'Skipped {self.skipped} non-land files.')\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx][0]\n",
        "        coordinates = (float(self.data[idx][1]), float(self.data[idx][2]))\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(coordinates, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jbTnwLFgCxQ"
      },
      "outputs": [],
      "source": [
        "# https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
        "# The mean and std of ImageNet are: mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
        "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((336, 336)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4tDw2LEPg_t"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX1CrLHEg22s"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageCoordinateDataset(csv_file='datasets/osv5m/train.csv', image_dirs=[\n",
        "                                      'datasets/osv5m/images/train/00',\n",
        "                                      ],\\\n",
        "                                 transform=transform)\n",
        "val_dataset = ImageCoordinateDataset(csv_file='datasets/osv5m/test.csv', image_dirs=[\n",
        "                                      'datasets/osv5m/images/test/00',\n",
        "                                      ],\\\n",
        "                                 transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWf20NGttGJ7"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('using cuda')\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('using cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvsrP4ZOlLZ1"
      },
      "outputs": [],
      "source": [
        "CLIP_MODEL = 'openai/clip-vit-large-patch14-336'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHjulkiPRSom"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification, CLIPVisionModel\n",
        "embed_model = CLIPVisionModel.from_pretrained(CLIP_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmaDlNR-R8Nn"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/geocells/\n",
        "# Political boundaries (admin0 = country)\n",
        "![ ! -f data/geocells/admin_2.geojson ] && wget --show-progress -O data/geocells/admin_2.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.geojson\n",
        "![ ! -f data/geocells/admin_1.geojson ] && wget --show-progress -O data/geocells/admin_1.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.geojson\n",
        "![ ! -f data/geocells/countries.geojson ] && wget --show-progress -O data/geocells/countries.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM0.geojson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pIAV0ZKqp8Y"
      },
      "outputs": [],
      "source": [
        "# TODO: pre-generate all this, add to the repo and add option to load instead\n",
        "import sys\n",
        "sys.path.append(f'{PIGEON_DIR}/dataset_creation/geocell')\n",
        "sys.path.append(PIGEON_DIR)\n",
        "\n",
        "from geocell_creation import *\n",
        "\n",
        "GEOCELL_PATH = 'data/geocells_yfcc.csv'\n",
        "df = train_dataset.df\n",
        "geocells_file = Path(GEOCELL_PATH)\n",
        "if not geocells_file.is_file():\n",
        "    geocell_creator = GeocellCreator(df, GEOCELL_PATH)\n",
        "    geocells = geocell_creator.generate()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q -U srtm-py georasters pandarallel latlon-utils pygeos datasets huggingface_hub"
      ],
      "metadata": {
        "id": "_UjD2eTu0OHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface-hub transformers pygeos"
      ],
      "metadata": {
        "id": "GP7MF8Zu-X63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U dill datasets"
      ],
      "metadata": {
        "id": "LTLch1GNJsSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PoObo_BC0nNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ66K8OwhA5A"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from collections import namedtuple\n",
        "from preprocessing import haversine_matrix, smooth_labels\n",
        "from models.layers import PositionalEncoder\n",
        "from models.utils import ModelOutput\n",
        "from config import *\n",
        "GEOCELL_PATH = 'data/geocells_yfcc.csv'\n",
        "\n",
        "#TODO: move over more stuff from PIGEON\n",
        "\n",
        "class GeoLocationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeoLocationModel, self).__init__()\n",
        "\n",
        "        self.panorama = False\n",
        "        # self.hidden_size = embed_dim\n",
        "        self.serving = False\n",
        "        self.should_smooth_labels = False\n",
        "        self.multi_task = False\n",
        "        # self.heading = heading\n",
        "        self.yfcc = None\n",
        "        self.freeze_base = False\n",
        "        self.hierarchical = False\n",
        "        # self.num_candidates = num_candidates\n",
        "\n",
        "        # Save variables\n",
        "        self.base_model = embed_model\n",
        "\n",
        "        resnet = torchvision.models.resnet50(pretrained=True).to(device)\n",
        "        for i, param in enumerate(resnet.parameters()):\n",
        "          if i < 4:\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        # Setup\n",
        "        self._set_hidden_size()\n",
        "        geocell_path = GEOCELL_PATH\n",
        "        self.lla_geocells = self.load_geocells(geocell_path)\n",
        "        self.num_cells = self.lla_geocells.size(0)\n",
        "\n",
        "        # Freeze / load parameters\n",
        "        # self._freeze_params()\n",
        "\n",
        "        # Loss\n",
        "        self.loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.base_model(pixel_values=x)\n",
        "        if self.mode == 'transformer':\n",
        "            embedding = embedding.last_hidden_state\n",
        "            embedding = torch.mean(embedding, dim=1)\n",
        "        else:\n",
        "            embedding = embedding.pooler_output\n",
        "        output = embedding\n",
        "\n",
        "        # Linear layer\n",
        "        logits = self.cell_layer(output)\n",
        "        geocell_probs = self.softmax(logits)\n",
        "\n",
        "        # Compute coordinate prediction\n",
        "        geocell_preds = torch.argmax(geocell_probs, dim=-1)\n",
        "        pred_LLH = torch.index_select(self.lla_geocells.data, 0, geocell_preds)\n",
        "        # label_probs = self._to_one_hot(labels_clf) # labels_clf if normal\n",
        "\n",
        "        # Get top 'num_candidates' geocell candidates\n",
        "        geocell_topk = torch.topk(geocell_probs, self.num_candidates, dim=-1)\n",
        "\n",
        "        # Soft labels based on distance\n",
        "        # if self.should_smooth_labels:\n",
        "            # distances = haversine_matrix(labels, self.lla_geocells.data.t())\n",
        "            # label_probs = smooth_labels(distances)\n",
        "\n",
        "        # Loss\n",
        "        # loss_clf = self.loss_fnc(logits, label_probs)\n",
        "\n",
        "        return pred_LLH\n",
        "\n",
        "    def load_geocells(self, path: str) -> Tensor:\n",
        "        \"\"\"Loads geocell centroids and converts them to ECEF format\n",
        "\n",
        "        Args:\n",
        "            path (str, optional): path to geocells. Defaults to GEOCELL_PATH.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: ECEF geocell centroids\n",
        "        \"\"\"\n",
        "        geo_df = pd.read_csv(path)\n",
        "        lla_coords = torch.tensor(geo_df[['longitude', 'latitude']].values)\n",
        "        lla_geocells = nn.parameter.Parameter(data=lla_coords, requires_grad=False)\n",
        "        return lla_geocells\n",
        "\n",
        "    def _set_hidden_size(self):\n",
        "        \"\"\"\n",
        "        Determines the hidden size of the model\n",
        "        \"\"\"\n",
        "        if self.base_model is not None:\n",
        "            try:\n",
        "                self.hidden_size = self.base_model.config.hidden_size\n",
        "                self.mode = 'transformer'\n",
        "\n",
        "            except AttributeError:\n",
        "                self.hidden_size = self.base_model.config.hidden_sizes[-1]\n",
        "                self.mode = 'convnext'\n",
        "\n",
        "    # def _freeze_params(self):\n",
        "    #     \"\"\"Freezes model parameters depending on mode\n",
        "    #     \"\"\"\n",
        "    #     if self.base_model is not None:\n",
        "    #         if self.freeze_base:\n",
        "    #             for param in self.base_model.parameters():\n",
        "    #                 param.requires_grad = False\n",
        "\n",
        "    #         # Load parameters and freeze relevant parameters\n",
        "    #         elif 'clip-vit' in self.base_model.config._name_or_path and not self.serving:\n",
        "    #             head = CLIP_PRETRAINED_HEAD_YFCC if self.yfcc else CLIP_PRETRAINED_HEAD\n",
        "    #             self.load_state(head)\n",
        "    #             print(f'Initialized model parameters from model: {head}')\n",
        "    #             for param in self.base_model.vision_model.encoder.layers[:-1].parameters():\n",
        "    #                 param.requires_grad = False\n",
        "\n",
        "model = GeoLocationModel()\n",
        "\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRXeAFEjwu4J"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHkhZVaFwglE"
      },
      "outputs": [],
      "source": [
        "clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
        "clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bMkIesfhgOx"
      },
      "outputs": [],
      "source": [
        "# https://github.com/gastruc/osv5m/blob/main/models/losses.py\n",
        "class HaversineLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HaversineLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: torch.Tensor Bx2\n",
        "            y: torch.Tensor Bx2\n",
        "        Returns:\n",
        "            torch.Tensor: Haversine loss between x and y: torch.Tensor([B])\n",
        "        Note:\n",
        "            Haversine distance doesn't contain the 2 * 6371 constant.\n",
        "        \"\"\"\n",
        "        lhs = torch.sin((x[:, 0] - y[:, 0]) / 2) ** 2\n",
        "        rhs = (\n",
        "            torch.cos(x[:, 0])\n",
        "            * torch.cos(y[:, 0])\n",
        "            * torch.sin((x[:, 1] - y[:, 1]) / 2) ** 2\n",
        "        )\n",
        "        a = lhs + rhs\n",
        "        return torch.arctan2(torch.sqrt(a), torch.sqrt(1 - a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEmS-g60i7Tt"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "haversineLoss = HaversineLoss()\n",
        "mseLoss = nn.MSELoss()\n",
        "ceLoss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.9, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qMwsMv4onU_"
      },
      "outputs": [],
      "source": [
        "!pip install -q livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAxnhJv7ogFl"
      },
      "outputs": [],
      "source": [
        "from livelossplot import PlotLosses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvBopkrnp7KE"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import Output\n",
        "OUTPUT_CONTEXT = Output()\n",
        "display(OUTPUT_CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPJ4TLwjNOi"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "avg_epoch_loss = []\n",
        "\n",
        "plotlosses = PlotLosses(figsize=(10, 5))\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), total=num_epochs, desc='Epochs'):\n",
        "  train_loss = []\n",
        "  avg_loss = 0\n",
        "\n",
        "  for images, coordinates in (pbar:= tqdm(train_dataloader, total=len(train_dataloader), desc=f'Epoch {epoch + 1} progress')):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      images, coordinates = images.cuda(), coordinates.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "\n",
        "    loss_haversine = haversineLoss(outputs, coordinates).mean(dim=-1)\n",
        "\n",
        "    mse = mseLoss(outputs, coordinates)\n",
        "    cross_entropy = ceLoss(outputs, coordinates)\n",
        "\n",
        "    pbar.set_postfix_str(f'Loss: {loss_haversine:.5f}; \\\n",
        "                               \\nOut: {outputs.mean(dim=0).tolist()}\\\n",
        "                               \\nExpected: {coordinates.mean(dim=0).tolist()}')\n",
        "    train_loss.append(loss_haversine.detach())\n",
        "\n",
        "    with OUTPUT_CONTEXT:\n",
        "      plotlosses.update({\n",
        "          'Haversine': loss_haversine.item(),\n",
        "          'MSE': mse.item(),\n",
        "          'Cross Entropy': cross_entropy.item()\n",
        "      })\n",
        "      plotlosses.send()\n",
        "\n",
        "    loss_haversine.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  scheduler.step()\n",
        "  avg_loss = sum(train_loss) / len(train_dataloader)\n",
        "  avg_epoch_loss.append(avg_loss)\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz1NAWazkQoy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "avg_epoch_loss = torch.tensor(avg_epoch_loss, device =  'cpu')\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(range(1, len(avg_epoch_loss) + 1), avg_epoch_loss, label='Loss', color='blue', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8he5flBklMG"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq-IkvuxklYk"
      },
      "outputs": [],
      "source": [
        "avg_val_haversine_loss = 0.0\n",
        "batch_haversine = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for validation\n",
        "    for images, coordinates in tqdm(val_dataloader, total=len(val_dataloader), desc=f'Validating'):\n",
        "        # Forward pass\n",
        "        images, coordinates = images.cuda(), coordinates.cuda()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss_haversine = haversineLoss(outputs, coordinates).mean(dim=-1)\n",
        "        batch_haversine.append(loss_haversine)\n",
        "        avg_val_haversine_loss += loss_haversine\n",
        "\n",
        "# Calculate average loss and haversine\n",
        "avg_val_haversine_loss /= len(val_dataloader)\n",
        "\n",
        "print(f'Validation Haversine Loss: {avg_val_haversine_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-u02WHQk-qI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "# Back from gpu\n",
        "batch_haversine = torch.tensor(batch_haversine, device =  'cpu')\n",
        "\n",
        "# Remove outliers\n",
        "batch_haversine = median_filter(batch_haversine, size=100)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.plot(range(1, len(batch_haversine) + 1), batch_haversine, label='Validation haversine', color='green', marker='o')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('haversine')\n",
        "plt.title('Validation haversine loss per Batch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi30A-cxlMyd"
      },
      "outputs": [],
      "source": [
        "from global_land_mask import globe\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
        "\n",
        "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "\n",
        "def plot_predictions(model, dataloader, num_samples=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, coordinates in dataloader:\n",
        "\n",
        "            images = images.cuda()\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            images = images.cpu()\n",
        "            outputs = outputs.cpu()\n",
        "\n",
        "            rand_index = random.sample(range(0, len(images) - 1), min(num_samples, len(images) - 1))\n",
        "\n",
        "            for i in range(min(num_samples, len(images))):\n",
        "                i = rand_index[i]\n",
        "\n",
        "                pred_lat, pred_lon = outputs[i].cpu().numpy()\n",
        "                true_lat, true_lon = coordinates[i].numpy()\n",
        "\n",
        "                haver_err = haversineLoss(\n",
        "                                      torch.tensor(np.array([[pred_lon, pred_lat]]), dtype=torch.float32).deg2rad(),\n",
        "                                      torch.tensor(np.array([[true_lon, true_lat]]), dtype=torch.float32).deg2rad(),\n",
        "                                      )\n",
        "                # Display the image\n",
        "                img = images[i]\n",
        "                img = unnormalize(img).permute(1, 2, 0).numpy()\n",
        "                img = np.clip(img, 0, 1)\n",
        "\n",
        "                plt.imshow(img)\n",
        "                plt.title(f'Pred: ({pred_lat:.4f}, {pred_lon:.4f})\\nTrue: ({true_lat:.4f}, {true_lon:.4f})\\n Haversine: {haver_err}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # World map for better understanding of how bad our prediction is\n",
        "                geometry = [Point(pred_lon, pred_lat), Point(true_lon, true_lat)]\n",
        "                geo_df = GeoDataFrame(geometry = geometry)\n",
        "                world = gpd.read_file(url)\n",
        "                geo_df.plot(ax=world.plot(color=\"lightgrey\", figsize=(10, 6)), marker='x', c=['red', 'green'], markersize=50);\n",
        "\n",
        "                plt.show()\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODbO7jg4nJYq"
      },
      "outputs": [],
      "source": [
        "plot_predictions(model, val_dataloader, num_samples=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de640296b9d645a29f9e83a202d84631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7004ff49524240a6b97fdd2475a232ef",
              "IPY_MODEL_14c0d34e54144ad0a3e96629d7f205b7",
              "IPY_MODEL_bf9b0bb780a248a3aa9399a76d8afc5e"
            ],
            "layout": "IPY_MODEL_6251678cf3444ae292c655cb494ae2aa"
          }
        },
        "7004ff49524240a6b97fdd2475a232ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2488262443454f5c86fbe9405d673374",
            "placeholder": "​",
            "style": "IPY_MODEL_fa9c2cd11bea4c6fb5fd29c4dea7035b",
            "value": " 50%"
          }
        },
        "14c0d34e54144ad0a3e96629d7f205b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da268ea87afd49139edbbde0d182428f",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b848f875d6484cbc892906fdde96a5e7",
            "value": 13
          }
        },
        "bf9b0bb780a248a3aa9399a76d8afc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc4cdf622c941d6a550988a73d9816e",
            "placeholder": "​",
            "style": "IPY_MODEL_e0c14be38d2e4e67b5f0fb21f4c80674",
            "value": " 13/26 [00:28&lt;00:28,  2.19s/it]"
          }
        },
        "6251678cf3444ae292c655cb494ae2aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2488262443454f5c86fbe9405d673374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9c2cd11bea4c6fb5fd29c4dea7035b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da268ea87afd49139edbbde0d182428f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b848f875d6484cbc892906fdde96a5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adc4cdf622c941d6a550988a73d9816e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c14be38d2e4e67b5f0fb21f4c80674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}