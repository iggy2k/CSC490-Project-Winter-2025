{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6vnZnI57N0t6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !tar -xf /content/drive/My\\ Drive/conda_colab.tar.gz -C ../\n",
        "\n",
        "# import os\n",
        "# path = '/content/miniconda3/bin:' + os.environ['PATH']\n",
        "# %env PATH=$path\n",
        "# %env PYTHONPATH=\n",
        "# import sys\n",
        "# _ = sys.path.append(\"/content/miniconda3/lib/python3.7/site-packages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Uq7mMKdJgOuA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# This is supposed to make sure if you edit .py files\n",
        "# colab will pick up on it\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS51rJiCGFvj",
        "outputId": "9a6d2988-805c-4368-dbed-c3d18c603027"
      },
      "outputs": [],
      "source": [
        "!if [[ ! -d PROJECT ]]; then git clone https://github.com/iggy2k/CSC490-Project-Winter-2025.git PROJECT; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28L_cjgBG57D",
        "outputId": "4e7ca638-6a48-418d-da43-638f7defdcde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/iggy/miniforge3/envs/3-8-10/lib/python38.zip', 'PROJECT/PIGEON', '/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8', '/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/lib-dynload', '', '/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages', '/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/setuptools/_vendor']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(1, 'PROJECT/PIGEON')\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYbBrphxG5wJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI20GDGtPm6G",
        "outputId": "9a3fe659-1495-4b3b-f5e7-adcfd460a6d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not in colab\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "try:\n",
        "    import condacolab\n",
        "    condacolab.install() # expect a kernel restart\n",
        "    IN_COLAB = True\n",
        "    os.environ[\"IN_COLAB\"] = \"1\"\n",
        "    print('in colab')\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print('not in colab')\n",
        "    os.unsetenv('IN_COLAB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oyvZd0SGqId",
        "outputId": "374e0aea-fafb-4fac-a768-6bc3f6d5dc06"
      },
      "outputs": [],
      "source": [
        "# Only use in colab\n",
        "!if [ -v IN_COLAB ]; then yes | mamba create -q --name 3-8-10 python=3.8.10; fi\n",
        "!if [ -v IN_COLAB ]; then eval \"$(conda shell.bash hook)\"; conda activate 3-8-10; python --version; fi\n",
        "!if [ -v IN_COLAB ]; then mamba env update -n 3-8-10 -f \"./PROJECT/environment.yaml\"; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pycountry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz4Nm0VPNvir"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !tar -zcf conda_colab.tar.gz /content/miniconda3\n",
        "# !cp conda_colab.tar.gz /content/drive/My\\ Drive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BpUyGTFsmpg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ILwLpBe2LK"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "332018d2257e4da2a137ec2116129217",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc3e7141ebc34616a92637137a474192",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv:   3%|2         | 83.9M/2.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "381ad7be721246cead4185a0895b874c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "00.zip:   4%|3         | 83.9M/2.25G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4bbe61467534c63bcd19c8ce1d29004",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv:  63%|######3   | 73.4M/116M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0193e8f3bba44fff808217eb22dc04f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "00.zip:   3%|2         | 73.4M/2.52G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/site-packages/tqdm/contrib/concurrent.py:76\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpool_kwargs) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/osv5m/train\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mosv5m/osv5m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/osv5m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages/train/00.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages/test/00.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/site-packages/huggingface_hub/_snapshot_download.py:296\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    294\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFetching \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/site-packages/tqdm/contrib/concurrent.py:94\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/site-packages/tqdm/contrib/concurrent.py:76\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     map_args\u001b[38;5;241m.\u001b[39mupdate(chunksize\u001b[38;5;241m=\u001b[39mchunksize)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpool_kwargs) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(tqdm_class(ex\u001b[38;5;241m.\u001b[39mmap(fn, \u001b[38;5;241m*\u001b[39miterables, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmap_args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/concurrent/futures/_base.py:644\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/concurrent/futures/thread.py:236\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 236\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
            "File \u001b[0;32m~/miniforge3/envs/3-8-10/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('datasets/osv5m/train'):\n",
        "    snapshot_download(repo_id=\"osv5m/osv5m\", local_dir=\"datasets/osv5m\", allow_patterns=[\n",
        "        'images/train/00.zip',\n",
        "        'images/test/00.zip',\n",
        "        '*.csv'\n",
        "        ], repo_type='dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: global-land-mask in /Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages (1.0.0)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q global-land-mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JxsgC4Ne7Tj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from time import sleep\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "from global_land_mask import globe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sk_1i7If3mt"
      },
      "outputs": [],
      "source": [
        "for root, dirs, files in os.walk(\"datasets/osv5m\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".zip\"):\n",
        "            with zipfile.ZipFile(os.path.join(root, file), 'r') as zip_ref:\n",
        "                for member in tqdm(zip_ref.infolist(), desc=f'Extracting {os.path.join(root, file)}'):\n",
        "                  zip_ref.extract(member, root)\n",
        "            os.remove(os.path.join(root, file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG-kxcaQfsgj"
      },
      "outputs": [],
      "source": [
        "import pycountry\n",
        "\n",
        "# Debug\n",
        "MAX_ITEMS = 1000\n",
        "\n",
        "class ImageCoordinateDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dirs, transform=None):\n",
        "        self.data = []\n",
        "        self.files = []\n",
        "        self.images = []\n",
        "        self.skipped = 0\n",
        "        self.csv = csv_file\n",
        "        self.transform = transform\n",
        "\n",
        "        for image_dir in image_dirs:\n",
        "          print('Reading', image_dir)\n",
        "\n",
        "          self.files.extend([f\"{image_dir}/{f}\" for f in listdir(image_dir) if isfile(join(image_dir, f))])\n",
        "\n",
        "          print(f'Found {len(self.files)} files.')\n",
        "\n",
        "          if not os.path.isdir('datasets/osv5m/'):\n",
        "            os.makedirs('datasets/osv5m/')\n",
        "          if isfile(f\"{csv_file}_filtered.csv\"):\n",
        "            self.df = pd.read_csv(f\"{csv_file}_filtered.csv\", index_col=False)\n",
        "          else:\n",
        "            self.df = pd.concat([chunk for chunk in tqdm(pd.read_csv(self.csv, chunksize=5000, usecols=['id', 'latitude', 'longitude', 'country'], index_col=False), desc='Loading data')])\n",
        "\n",
        "          print(f'Found {len(self.df)} csv entries.')\n",
        "\n",
        "          self.df['country'] = self.df['country'].apply(lambda x: pycountry.countries.get(alpha_2=x).name if pycountry.countries.get(alpha_2=x) else x)\n",
        "\n",
        "          new = pd.DataFrame(columns=['id', 'latitude', 'longitude', 'country'])\n",
        "          i = 0\n",
        "          for full_path in tqdm(self.files, total=len(self.files), desc='Processing files'):\n",
        "              image_name = str(Path(full_path).stem)\n",
        "\n",
        "              try:\n",
        "                row = self.df[self.df['id'] == int(image_name)].iloc[0]\n",
        "              except:\n",
        "                pass\n",
        "              new.loc[i] = row\n",
        "              lat = row['latitude']\n",
        "              lon = row['longitude']\n",
        "\n",
        "              # Remove mislaballed images (ocean pictures?)\n",
        "              # if not globe.is_land(float(lat), float(lon)):\n",
        "              #   self.skipped += 1\n",
        "              #   continue\n",
        "              self.data.append(np.array([str(full_path), float(lat), float(lon)]))\n",
        "              i += 1\n",
        "\n",
        "          self.df = new\n",
        "        if MAX_ITEMS is not None:\n",
        "          print(f'Keeping {MAX_ITEMS} items')\n",
        "          self.df = self.df[:MAX_ITEMS]\n",
        "          self.data = self.data[:MAX_ITEMS]\n",
        "          self.files = self.files[:MAX_ITEMS]\n",
        "\n",
        "        self.df.to_csv(f\"{csv_file}_filtered.csv\", index=False)\n",
        "\n",
        "        print(f'Dataset ready, {len(self.files)} files.')\n",
        "        print(f'Skipped {self.skipped} non-land files.')\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx][0]\n",
        "        coordinates = (float(self.data[idx][1]), float(self.data[idx][2]))\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(coordinates, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jbTnwLFgCxQ"
      },
      "outputs": [],
      "source": [
        "# https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
        "# The mean and std of ImageNet are: mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
        "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4tDw2LEPg_t"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX1CrLHEg22s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading datasets/osv5m/images/train/00\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datasets/osv5m/images/train/00'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageCoordinateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/osv5m/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/osv5m/images/train/00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# val_dataset = ImageCoordinateDataset(csv_file='datasets/osv5m/test.csv', image_dirs=[\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#                                       'datasets/osv5m/images/test/00',\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#                                       ],\\\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#                                  transform=transform)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mImageCoordinateDataset.__init__\u001b[0;34m(self, csv_file, image_dirs, transform)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_dir \u001b[38;5;129;01min\u001b[39;00m image_dirs:\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading\u001b[39m\u001b[38;5;124m'\u001b[39m, image_dir)\n\u001b[0;32m---> 18\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m isfile(join(image_dir, f))])\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/osv5m/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/osv5m/images/train/00'"
          ]
        }
      ],
      "source": [
        "train_dataset = ImageCoordinateDataset(csv_file='datasets/osv5m/train.csv', image_dirs=[\n",
        "                                      'datasets/osv5m/images/train/00',\n",
        "                                      ],\\\n",
        "                                 transform=transform)\n",
        "val_dataset = ImageCoordinateDataset(csv_file='datasets/osv5m/test.csv', image_dirs=[\n",
        "                                      'datasets/osv5m/images/test/00',\n",
        "                                      ],\\\n",
        "                                 transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWf20NGttGJ7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('using cuda')\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('using cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvsrP4ZOlLZ1"
      },
      "outputs": [],
      "source": [
        "CLIP_MODEL = 'openai/clip-vit-large-patch14-336'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHjulkiPRSom"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification, CLIPVisionModel\n",
        "embed_model = CLIPVisionModel.from_pretrained(CLIP_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmaDlNR-R8Nn"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/geocells/\n",
        "# Political boundaries (admin0 = country)\n",
        "![ ! -f data/geocells/admin_2.geojson ] && wget --show-progress -O data/geocells/admin_2.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.geojson\n",
        "![ ! -f data/geocells/admin_1.geojson ] && wget --show-progress -O data/geocells/admin_1.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.geojson\n",
        "![ ! -f data/geocells/countries.geojson ] && wget --show-progress -O data/geocells/countries.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM0.geojson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys5aMHucGqIk"
      },
      "outputs": [],
      "source": [
        "GEOCELL_PATH = 'data/geocells_yfcc.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pIAV0ZKqp8Y"
      },
      "outputs": [],
      "source": [
        "# TODO: pre-generate all this, add to the repo and add option to load instead\n",
        "import sys\n",
        "sys.path.insert(0,'PROJECT/PIGEON/dataset_creation/geocell')\n",
        "sys.path.insert(0,'PROJECT/PIGEON/')\n",
        "from geocell_creation import *\n",
        "\n",
        "df = train_dataset.df\n",
        "geocells_file = Path(GEOCELL_PATH)\n",
        "if not geocells_file.is_file():\n",
        "    geocell_creator = GeocellCreator(df, GEOCELL_PATH)\n",
        "    geocells = geocell_creator.generate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ66K8OwhA5A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1184, in _get_module\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/models/auto/feature_extraction_auto.py\", line 25, in <module>\n",
            "    from ...feature_extraction_utils import FeatureExtractionMixin\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/feature_extraction_utils.py\", line 28, in <module>\n",
            "    from .utils import (\n",
            "ImportError: cannot import name 'torch_required' from 'transformers.utils' (/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/utils/__init__.py)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
            "    Parameters\n",
            "  File \"/var/folders/kp/c9v12zrn6g90b3mzyhbrp74m0000gn/T/ipykernel_34289/139244993.py\", line 9, in <module>\n",
            "    from preprocessing import haversine_matrix, smooth_labels\n",
            "  File \"/Users/iggy/Documents/GitHub/CSC490-Project-Winter-2025/testing/PROJECT/PIGEON/preprocessing/__init__.py\", line 3, in <module>\n",
            "    from .dataset_preprocessing import *\n",
            "  File \"/Users/iggy/Documents/GitHub/CSC490-Project-Winter-2025/testing/PROJECT/PIGEON/preprocessing/dataset_preprocessing.py\", line 14, in <module>\n",
            "    from transformers import AutoFeatureExtractor\n",
            "  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1175, in __getattr__\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1174, in __getattr__\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/utils/import_utils.py\", line 1186, in _get_module\n",
            "RuntimeError: Failed to import transformers.models.auto.feature_extraction_auto because of the following error (look up to see its traceback):\n",
            "cannot import name 'torch_required' from 'transformers.utils' (/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/utils/__init__.py)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
            "    # Add custom completers to the basic ones built into IPCompleter\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
            "    def __call__(self, etype, value, elist):\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
            "    # Changed so an instance can just be called as VerboseTB_inst() and print\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 778, in lines\n",
            "    evalue: BaseException,\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
            "    pieces = self.included_pieces\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
            "    pos = scope_pieces.index(self.executing_piece)\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
            "    return only(\n",
            "  File \"/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/executing/executing.py\", line 116, in only\n",
            "    raise NotOneValueFound('Expected one value, found 0')\n",
            "executing.executing.NotOneValueFound: Expected one value, found 0\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from collections import namedtuple\n",
        "from preprocessing import haversine_matrix, smooth_labels\n",
        "from models.layers import PositionalEncoder\n",
        "from models.utils import ModelOutput\n",
        "from config import *\n",
        "\n",
        "#TODO: move over more stuff from PIGEON\n",
        "\n",
        "class GeoLocationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeoLocationModel, self).__init__()\n",
        "\n",
        "        # Save variables\n",
        "        self.base_model = embed_model\n",
        "\n",
        "        resnet = torchvision.models.resnet50(pretrained=True).to(device)\n",
        "        for i, param in enumerate(resnet.parameters()):\n",
        "          if i < 4:\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        # Setup\n",
        "        self._set_hidden_size()\n",
        "        geocell_path = GEOCELL_PATH\n",
        "        self.lla_geocells = self.load_geocells(geocell_path)\n",
        "        self.num_cells = self.lla_geocells.size(0)\n",
        "\n",
        "        self.resnet = resnet\n",
        "        num_feats = self.resnet.fc.in_features\n",
        "        self.fc1 = nn.Linear(num_feats, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.resnet(x)\n",
        "\n",
        "        x = self.fc1(features)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def load_geocells(self, path: str) -> Tensor:\n",
        "        \"\"\"Loads geocell centroids and converts them to ECEF format\n",
        "\n",
        "        Args:\n",
        "            path (str, optional): path to geocells. Defaults to GEOCELL_PATH.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: ECEF geocell centroids\n",
        "        \"\"\"\n",
        "        geo_df = pd.read_csv(path)\n",
        "        lla_coords = torch.tensor(geo_df[['longitude', 'lat']].values)\n",
        "        lla_geocells = nn.parameter.Parameter(data=lla_coords, requires_grad=False)\n",
        "        return lla_geocells\n",
        "\n",
        "    def _set_hidden_size(self):\n",
        "        \"\"\"\n",
        "        Determines the hidden size of the model\n",
        "        \"\"\"\n",
        "        if self.base_model is not None:\n",
        "            try:\n",
        "                self.hidden_size = self.base_model.config.hidden_size\n",
        "                self.mode = 'transformer'\n",
        "\n",
        "            except AttributeError:\n",
        "                self.hidden_size = self.base_model.config.hidden_sizes[-1]\n",
        "                self.mode = 'convnext'\n",
        "\n",
        "model = GeoLocationModel()\n",
        "\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRXeAFEjwu4J"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHkhZVaFwglE"
      },
      "outputs": [],
      "source": [
        "clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
        "clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bMkIesfhgOx"
      },
      "outputs": [],
      "source": [
        "# https://github.com/gastruc/osv5m/blob/main/models/losses.py\n",
        "class HaversineLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HaversineLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: torch.Tensor Bx2\n",
        "            y: torch.Tensor Bx2\n",
        "        Returns:\n",
        "            torch.Tensor: Haversine loss between x and y: torch.Tensor([B])\n",
        "        Note:\n",
        "            Haversine distance doesn't contain the 2 * 6371 constant.\n",
        "        \"\"\"\n",
        "        lhs = torch.sin((x[:, 0] - y[:, 0]) / 2) ** 2\n",
        "        rhs = (\n",
        "            torch.cos(x[:, 0])\n",
        "            * torch.cos(y[:, 0])\n",
        "            * torch.sin((x[:, 1] - y[:, 1]) / 2) ** 2\n",
        "        )\n",
        "        a = lhs + rhs\n",
        "        return torch.arctan2(torch.sqrt(a), torch.sqrt(1 - a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEmS-g60i7Tt"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "haversineLoss = HaversineLoss()\n",
        "mseLoss = nn.MSELoss()\n",
        "ceLoss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.9, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qMwsMv4onU_"
      },
      "outputs": [],
      "source": [
        "!pip install -q livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAxnhJv7ogFl"
      },
      "outputs": [],
      "source": [
        "from livelossplot import PlotLosses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvBopkrnp7KE"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import Output\n",
        "OUTPUT_CONTEXT = Output()\n",
        "display(OUTPUT_CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPJ4TLwjNOi"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "avg_epoch_loss = []\n",
        "\n",
        "plotlosses = PlotLosses(figsize=(10, 5))\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), total=num_epochs, desc='Epochs'):\n",
        "  train_loss = []\n",
        "  avg_loss = 0\n",
        "\n",
        "  for images, coordinates in (pbar:= tqdm(train_dataloader, total=len(train_dataloader), desc=f'Epoch {epoch + 1} progress')):\n",
        "    images, coordinates = images.cuda(), coordinates.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "\n",
        "    loss_haversine = haversineLoss(outputs, coordinates).mean(dim=-1)\n",
        "\n",
        "    mse = mseLoss(outputs, coordinates)\n",
        "    cross_entropy = ceLoss(outputs, coordinates)\n",
        "\n",
        "    pbar.set_postfix_str(f'Loss: {loss_haversine:.5f}; \\\n",
        "                               \\nOut: {outputs.mean(dim=0).tolist()}\\\n",
        "                               \\nExpected: {coordinates.mean(dim=0).tolist()}')\n",
        "    train_loss.append(loss_haversine.detach())\n",
        "\n",
        "    with OUTPUT_CONTEXT:\n",
        "      plotlosses.update({\n",
        "          'Haversine': loss_haversine.item(),\n",
        "          'MSE': mse.item(),\n",
        "          'Cross Entropy': cross_entropy.item()\n",
        "      })\n",
        "      plotlosses.send()\n",
        "\n",
        "    loss_haversine.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  scheduler.step()\n",
        "  avg_loss = sum(train_loss) / len(train_dataloader)\n",
        "  avg_epoch_loss.append(avg_loss)\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz1NAWazkQoy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "avg_epoch_loss = torch.tensor(avg_epoch_loss, device =  'cpu')\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(range(1, len(avg_epoch_loss) + 1), avg_epoch_loss, label='Loss', color='blue', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8he5flBklMG"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq-IkvuxklYk"
      },
      "outputs": [],
      "source": [
        "avg_val_haversine_loss = 0.0\n",
        "batch_haversine = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for validation\n",
        "    for images, coordinates in tqdm(val_dataloader, total=len(val_dataloader), desc=f'Validating'):\n",
        "        # Forward pass\n",
        "        images, coordinates = images.cuda(), coordinates.cuda()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss_haversine = haversineLoss(outputs, coordinates).mean(dim=-1)\n",
        "        batch_haversine.append(loss_haversine)\n",
        "        avg_val_haversine_loss += loss_haversine\n",
        "\n",
        "# Calculate average loss and haversine\n",
        "avg_val_haversine_loss /= len(val_dataloader)\n",
        "\n",
        "print(f'Validation Haversine Loss: {avg_val_haversine_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-u02WHQk-qI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "# Back from gpu\n",
        "batch_haversine = torch.tensor(batch_haversine, device =  'cpu')\n",
        "\n",
        "# Remove outliers\n",
        "batch_haversine = median_filter(batch_haversine, size=100)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.plot(range(1, len(batch_haversine) + 1), batch_haversine, label='Validation haversine', color='green', marker='o')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('haversine')\n",
        "plt.title('Validation haversine loss per Batch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi30A-cxlMyd"
      },
      "outputs": [],
      "source": [
        "# Step 5: Visualize some predictions (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
        "\n",
        "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "\n",
        "def plot_predictions(model, dataloader, num_samples=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, coordinates in dataloader:\n",
        "\n",
        "            images = images.cuda()\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            images = images.cpu()\n",
        "            outputs = outputs.cpu()\n",
        "\n",
        "            rand_index = random.sample(range(0, len(images) - 1), min(num_samples, len(images) - 1))\n",
        "\n",
        "            for i in range(min(num_samples, len(images))):\n",
        "                i = rand_index[i]\n",
        "\n",
        "                pred_lat, pred_lon = outputs[i].cpu().numpy()\n",
        "                true_lat, true_lon = coordinates[i].numpy()\n",
        "\n",
        "                haver_err = haversineLoss(\n",
        "                                      torch.tensor(np.array([[pred_lon, pred_lat]]), dtype=torch.float32).deg2rad(),\n",
        "                                      torch.tensor(np.array([[true_lon, true_lat]]), dtype=torch.float32).deg2rad(),\n",
        "                                      )\n",
        "                # Display the image\n",
        "                img = images[i]\n",
        "                img = unnormalize(img).permute(1, 2, 0).numpy()\n",
        "                img = np.clip(img, 0, 1)\n",
        "\n",
        "                plt.imshow(img)\n",
        "                plt.title(f'Pred: ({pred_lat:.4f}, {pred_lon:.4f})\\nTrue: ({true_lat:.4f}, {true_lon:.4f})\\n Haversine: {haver_err}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # World map for better understanding of how bad our prediction is\n",
        "                geometry = [Point(pred_lon, pred_lat), Point(true_lon, true_lat)]\n",
        "                geo_df = GeoDataFrame(geometry = geometry)\n",
        "                world = gpd.read_file(url)\n",
        "                geo_df.plot(ax=world.plot(color=\"lightgrey\", figsize=(10, 6)), marker='x', c=['red', 'green'], markersize=50);\n",
        "\n",
        "                plt.show()\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODbO7jg4nJYq"
      },
      "outputs": [],
      "source": [
        "plot_predictions(model, val_dataloader, num_samples=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
