{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6vnZnI57N0t6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !tar -xf /content/drive/My\\ Drive/conda_colab.tar.gz -C ../\n",
        "\n",
        "# import os\n",
        "# path = '/content/miniconda3/bin:' + os.environ['PATH']\n",
        "# %env PATH=$path\n",
        "# %env PYTHONPATH=\n",
        "# import sys\n",
        "# _ = sys.path.append(\"/content/miniconda3/lib/python3.7/site-packages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uq7mMKdJgOuA"
      },
      "outputs": [],
      "source": [
        "# This is supposed to make sure if you edit .py files\n",
        "# colab will pick up on it\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Local\n",
        "if os.path.isdir('PIGEON/'):\n",
        "    PIGEON_DIR = 'PIGEON/'\n",
        "# Git clone when colab\n",
        "else:\n",
        "    PIGEON_DIR = 'PROJECT/PIGEON/'\n",
        "\n",
        "os.environ[\"PIGEON_DIR\"] = PIGEON_DIR\n",
        "sys.path.insert(1, PIGEON_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS51rJiCGFvj",
        "outputId": "9a6d2988-805c-4368-dbed-c3d18c603027"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "! was unexpected at this time.\n"
          ]
        }
      ],
      "source": [
        "!if [[ ! -d PROJECT ]] && [[ ! -d PIGEON ]]; then git clone https://github.com/iggy2k/CSC490-Project-Winter-2025.git PROJECT; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hYbBrphxG5wJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI20GDGtPm6G",
        "outputId": "9a3fe659-1495-4b3b-f5e7-adcfd460a6d0"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# try:\n",
        "#     import condacolab\n",
        "#     condacolab.install() # expect a kernel restart\n",
        "#     IN_COLAB = True\n",
        "#     os.environ[\"IN_COLAB\"] = \"1\"\n",
        "#     print('in colab')\n",
        "# except:\n",
        "#     IN_COLAB = False\n",
        "#     print('not in colab')\n",
        "#     os.unsetenv('IN_COLAB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oyvZd0SGqId",
        "outputId": "374e0aea-fafb-4fac-a768-6bc3f6d5dc06"
      },
      "outputs": [],
      "source": [
        "# # Only use in colab\n",
        "# !if [ -v IN_COLAB ]; then yes | mamba create -q --name 3-8-10 python=3.8.10; fi\n",
        "# !if [ -v IN_COLAB ]; then eval \"$(conda shell.bash hook)\"; conda activate 3-8-10; python --version; fi\n",
        "# !if [ -v IN_COLAB ]; then mamba env update -n 3-8-10 -f \"./PROJECT/environment.yaml\"; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pycountry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rz4Nm0VPNvir"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !tar -zcf conda_colab.tar.gz /content/miniconda3\n",
        "# !cp conda_colab.tar.gz /content/drive/My\\ Drive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (0.29.1)\n",
            "Requirement already satisfied: transformers in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (4.46.3)\n",
            "Requirement already satisfied: requests in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from huggingface_hub) (4.64.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from huggingface_hub) (2025.2.0)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from transformers) (1.23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: colorama in c:\\users\\bfong\\anaconda3\\envs\\3-8-10\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
            "huggingface_hub version: 0.29.1\n",
            "transformers version: 4.46.3\n",
            "requests version: 2.32.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub transformers requests\n",
        "import huggingface_hub\n",
        "import transformers\n",
        "import requests\n",
        "\n",
        "print(\"huggingface_hub version:\", huggingface_hub.__version__)\n",
        "print(\"transformers version:\", transformers.__version__)\n",
        "print(\"requests version:\", requests.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BpUyGTFsmpg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ILwLpBe2LK"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d5be7943ed64b4aa4d3389981829c63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('datasets/osv5m/images'):\n",
        "    snapshot_download(repo_id=\"osv5m/osv5m\", local_dir=\"datasets/osv5m\", allow_patterns=[\n",
        "        'images/train/00.zip',\n",
        "        'images/test/00.zip',\n",
        "        '*.csv'\n",
        "        ], repo_type='dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q global-land-mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_JxsgC4Ne7Tj"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from time import sleep\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "from global_land_mask import globe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5sk_1i7If3mt"
      },
      "outputs": [],
      "source": [
        "for root, dirs, files in os.walk(\"datasets/osv5m\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".zip\"):\n",
        "            with zipfile.ZipFile(os.path.join(root, file), 'r') as zip_ref:\n",
        "                for member in tqdm(zip_ref.infolist(), desc=f'Extracting {os.path.join(root, file)}'):\n",
        "                  zip_ref.extract(member, root)\n",
        "            os.remove(os.path.join(root, file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OG-kxcaQfsgj"
      },
      "outputs": [],
      "source": [
        "import pycountry\n",
        "\n",
        "# Debug\n",
        "MAX_ITEMS = 1000\n",
        "\n",
        "class ImageCoordinateDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dirs, transform=None):\n",
        "        self.data = []\n",
        "        self.files = []\n",
        "        self.images = []\n",
        "        self.skipped = 0\n",
        "        self.csv = csv_file\n",
        "        self.transform = transform\n",
        "\n",
        "        for image_dir in image_dirs:\n",
        "          print('Reading', image_dir)\n",
        "\n",
        "          self.files.extend([f\"{image_dir}/{f}\" for f in listdir(image_dir) if isfile(join(image_dir, f))])\n",
        "\n",
        "          print(f'Found {len(self.files)} files.')\n",
        "\n",
        "          if not os.path.isdir('datasets/osv5m/'):\n",
        "            os.makedirs('datasets/osv5m/')\n",
        "          if isfile(f\"{csv_file}_filtered.csv\"):\n",
        "            self.df = pd.read_csv(f\"{csv_file}_filtered.csv\", index_col=False)\n",
        "          else:\n",
        "            self.df = pd.concat([chunk for chunk in tqdm(pd.read_csv(self.csv, chunksize=5000, usecols=['id', 'latitude', 'longitude', 'country'], index_col=False), desc='Loading data')])\n",
        "\n",
        "          print(f'Found {len(self.df)} csv entries.')\n",
        "\n",
        "          self.df['country'] = self.df['country'].apply(lambda x: pycountry.countries.get(alpha_2=x).name if pycountry.countries.get(alpha_2=x) else x)\n",
        "\n",
        "          new = pd.DataFrame(columns=['id', 'latitude', 'longitude', 'country'])\n",
        "          i = 0\n",
        "          for full_path in tqdm(self.files, total=len(self.files), desc='Processing files'):\n",
        "              image_name = str(Path(full_path).stem)\n",
        "\n",
        "              try:\n",
        "                row = self.df[self.df['id'] == int(image_name)].iloc[0]\n",
        "              except:\n",
        "                continue\n",
        "              new.loc[i] = row\n",
        "              lat = row['latitude']\n",
        "              lon = row['longitude']\n",
        "\n",
        "              # Remove mislaballed images (ocean pictures?)\n",
        "              # if not globe.is_land(float(lat), float(lon)):\n",
        "              #   self.skipped += 1\n",
        "              #   continue\n",
        "              self.data.append(np.array([str(full_path), float(lat), float(lon)]))\n",
        "              i += 1\n",
        "\n",
        "          self.df = new\n",
        "        if MAX_ITEMS is not None:\n",
        "          print(f'Keeping {MAX_ITEMS} items')\n",
        "          self.df = self.df[:MAX_ITEMS]\n",
        "          self.data = self.data[:MAX_ITEMS]\n",
        "          self.files = self.files[:MAX_ITEMS]\n",
        "\n",
        "        self.df.to_csv(f\"{csv_file}_filtered.csv\", index=False)\n",
        "\n",
        "        print(f'Dataset ready, {len(self.files)} files.')\n",
        "        print(f'Skipped {self.skipped} non-land files.')\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx][0]\n",
        "        coordinates = (float(self.data[idx][1]), float(self.data[idx][2]))\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(coordinates, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6jbTnwLFgCxQ"
      },
      "outputs": [],
      "source": [
        "# https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
        "# The mean and std of ImageNet are: mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
        "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((336, 336)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "t4tDw2LEPg_t"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nX1CrLHEg22s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading datasets/osv5m/images/train/00\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: 'datasets/osv5m/images/train/00'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageCoordinateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/osv5m/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/osv5m/images/train/00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ImageCoordinateDataset(csv_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/osv5m/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, image_dirs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      6\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/osv5m/images/test/00\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                       ],\\\n\u001b[0;32m      8\u001b[0m                                  transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     10\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mImageCoordinateDataset.__init__\u001b[1;34m(self, csv_file, image_dirs, transform)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_dir \u001b[38;5;129;01min\u001b[39;00m image_dirs:\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading\u001b[39m\u001b[38;5;124m'\u001b[39m, image_dir)\n\u001b[1;32m---> 18\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m isfile(join(image_dir, f))])\n\u001b[0;32m     20\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/osv5m/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'datasets/osv5m/images/train/00'"
          ]
        }
      ],
      "source": [
        "train_dataset = ImageCoordinateDataset(csv_file='datasets/osv5m/train.csv', image_dirs=[\n",
        "                                      'datasets/osv5m/images/train/00',\n",
        "                                      ],\\\n",
        "                                 transform=transform)\n",
        "val_dataset = ImageCoordinateDataset(csv_file='datasets/osv5m/test.csv', image_dirs=[\n",
        "                                      'datasets/osv5m/images/test/00',\n",
        "                                      ],\\\n",
        "                                 transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWf20NGttGJ7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('using cuda')\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('using cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvsrP4ZOlLZ1"
      },
      "outputs": [],
      "source": [
        "CLIP_MODEL = 'openai/clip-vit-large-patch14-336'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHjulkiPRSom"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification, CLIPVisionModel\n",
        "embed_model = CLIPVisionModel.from_pretrained(CLIP_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmaDlNR-R8Nn"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/geocells/\n",
        "# Political boundaries (admin0 = country)\n",
        "![ ! -f data/geocells/admin_2.geojson ] && wget --show-progress -O data/geocells/admin_2.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.geojson\n",
        "![ ! -f data/geocells/admin_1.geojson ] && wget --show-progress -O data/geocells/admin_1.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.geojson\n",
        "![ ! -f data/geocells/countries.geojson ] && wget --show-progress -O data/geocells/countries.geojson -q https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM0.geojson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pIAV0ZKqp8Y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# TODO: pre-generate all this, add to the repo and add option to load instead\n",
        "import sys\n",
        "sys.path.append(f'{PIGEON_DIR}/dataset_creation/geocell')\n",
        "sys.path.append(PIGEON_DIR)\n",
        "\n",
        "from geocell_creation import *\n",
        "\n",
        "GEOCELL_PATH = 'data/geocells_yfcc.csv'\n",
        "df = train_dataset.df\n",
        "geocells_file = Path(GEOCELL_PATH)\n",
        "if not geocells_file.is_file():\n",
        "    geocell_creator = GeocellCreator(df, GEOCELL_PATH)\n",
        "    geocells = geocell_creator.generate()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ66K8OwhA5A"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/libpysal/cg/alpha_shapes.py:39: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def nb_dist(x, y):\n",
            "/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/libpysal/cg/alpha_shapes.py:165: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def get_faces(triangle):\n",
            "/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/libpysal/cg/alpha_shapes.py:199: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\n",
            "/Users/iggy/miniforge3/envs/3-8-10/lib/python3.8/site-packages/libpysal/cg/alpha_shapes.py:261: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
            "  def nb_mask_faces(mask, faces):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 8 workers.\n",
            "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from collections import namedtuple\n",
        "from preprocessing import haversine_matrix, smooth_labels\n",
        "from models.layers import PositionalEncoder\n",
        "from models.utils import ModelOutput\n",
        "from config import *\n",
        "GEOCELL_PATH = 'data/geocells_yfcc.csv'\n",
        "\n",
        "#TODO: move over more stuff from PIGEON\n",
        "\n",
        "class GeoLocationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeoLocationModel, self).__init__()\n",
        "\n",
        "        self.panorama = False\n",
        "        # self.hidden_size = embed_dim\n",
        "        self.serving = False\n",
        "        self.should_smooth_labels = False\n",
        "        self.multi_task = False\n",
        "        # self.heading = heading\n",
        "        self.yfcc = None\n",
        "        self.freeze_base = False\n",
        "        self.hierarchical = False\n",
        "        # self.num_candidates = num_candidates\n",
        "\n",
        "        # Save variables\n",
        "        self.base_model = embed_model\n",
        "\n",
        "        resnet = torchvision.models.resnet50(pretrained=True).to(device)\n",
        "        for i, param in enumerate(resnet.parameters()):\n",
        "          if i < 4:\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "        # Setup\n",
        "        self._set_hidden_size()\n",
        "        geocell_path = GEOCELL_PATH\n",
        "        self.lla_geocells = self.load_geocells(geocell_path)\n",
        "        self.num_cells = self.lla_geocells.size(0)\n",
        "\n",
        "        # Freeze / load parameters\n",
        "        # self._freeze_params()\n",
        "\n",
        "        # Loss\n",
        "        self.loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.base_model(pixel_values=x)\n",
        "        if self.mode == 'transformer':\n",
        "            embedding = embedding.last_hidden_state\n",
        "            embedding = torch.mean(embedding, dim=1)\n",
        "        else:\n",
        "            embedding = embedding.pooler_output\n",
        "        output = embedding\n",
        "\n",
        "        # Linear layer\n",
        "        logits = self.cell_layer(output)\n",
        "        geocell_probs = self.softmax(logits)            \n",
        "\n",
        "        # Compute coordinate prediction\n",
        "        geocell_preds = torch.argmax(geocell_probs, dim=-1)\n",
        "        pred_LLH = torch.index_select(self.lla_geocells.data, 0, geocell_preds)\n",
        "        # label_probs = self._to_one_hot(labels_clf) # labels_clf if normal\n",
        "\n",
        "        # Get top 'num_candidates' geocell candidates\n",
        "        geocell_topk = torch.topk(geocell_probs, self.num_candidates, dim=-1)\n",
        "\n",
        "        # Soft labels based on distance\n",
        "        # if self.should_smooth_labels:\n",
        "            # distances = haversine_matrix(labels, self.lla_geocells.data.t())\n",
        "            # label_probs = smooth_labels(distances)\n",
        "\n",
        "        # Loss\n",
        "        # loss_clf = self.loss_fnc(logits, label_probs)\n",
        "\n",
        "        return pred_LLH\n",
        "\n",
        "    def load_geocells(self, path: str) -> Tensor:\n",
        "        \"\"\"Loads geocell centroids and converts them to ECEF format\n",
        "\n",
        "        Args:\n",
        "            path (str, optional): path to geocells. Defaults to GEOCELL_PATH.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: ECEF geocell centroids\n",
        "        \"\"\"\n",
        "        geo_df = pd.read_csv(path)\n",
        "        lla_coords = torch.tensor(geo_df[['longitude', 'latitude']].values)\n",
        "        lla_geocells = nn.parameter.Parameter(data=lla_coords, requires_grad=False)\n",
        "        return lla_geocells\n",
        "\n",
        "    def _set_hidden_size(self):\n",
        "        \"\"\"\n",
        "        Determines the hidden size of the model\n",
        "        \"\"\"\n",
        "        if self.base_model is not None:\n",
        "            try:\n",
        "                self.hidden_size = self.base_model.config.hidden_size\n",
        "                self.mode = 'transformer'\n",
        "\n",
        "            except AttributeError:\n",
        "                self.hidden_size = self.base_model.config.hidden_sizes[-1]\n",
        "                self.mode = 'convnext'\n",
        "\n",
        "    # def _freeze_params(self):\n",
        "    #     \"\"\"Freezes model parameters depending on mode\n",
        "    #     \"\"\"\n",
        "    #     if self.base_model is not None:\n",
        "    #         if self.freeze_base:\n",
        "    #             for param in self.base_model.parameters():\n",
        "    #                 param.requires_grad = False\n",
        "\n",
        "    #         # Load parameters and freeze relevant parameters\n",
        "    #         elif 'clip-vit' in self.base_model.config._name_or_path and not self.serving:\n",
        "    #             head = CLIP_PRETRAINED_HEAD_YFCC if self.yfcc else CLIP_PRETRAINED_HEAD\n",
        "    #             self.load_state(head)\n",
        "    #             print(f'Initialized model parameters from model: {head}')\n",
        "    #             for param in self.base_model.vision_model.encoder.layers[:-1].parameters():\n",
        "    #                 param.requires_grad = False\n",
        "\n",
        "model = GeoLocationModel()\n",
        "\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRXeAFEjwu4J"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHkhZVaFwglE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 768)\n",
              "      (position_embedding): Embedding(77, 768)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "      (position_embedding): Embedding(257, 1024)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (12): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (13): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (14): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (15): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (16): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (17): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (18): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (19): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (20): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (21): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (22): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (23): CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
              "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
        "clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bMkIesfhgOx"
      },
      "outputs": [],
      "source": [
        "# https://github.com/gastruc/osv5m/blob/main/models/losses.py\n",
        "class HaversineLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HaversineLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: torch.Tensor Bx2\n",
        "            y: torch.Tensor Bx2\n",
        "        Returns:\n",
        "            torch.Tensor: Haversine loss between x and y: torch.Tensor([B])\n",
        "        Note:\n",
        "            Haversine distance doesn't contain the 2 * 6371 constant.\n",
        "        \"\"\"\n",
        "        lhs = torch.sin((x[:, 0] - y[:, 0]) / 2) ** 2\n",
        "        rhs = (\n",
        "            torch.cos(x[:, 0])\n",
        "            * torch.cos(y[:, 0])\n",
        "            * torch.sin((x[:, 1] - y[:, 1]) / 2) ** 2\n",
        "        )\n",
        "        a = lhs + rhs\n",
        "        return torch.arctan2(torch.sqrt(a), torch.sqrt(1 - a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEmS-g60i7Tt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-02.\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "haversineLoss = HaversineLoss()\n",
        "mseLoss = nn.MSELoss()\n",
        "ceLoss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.9, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qMwsMv4onU_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAxnhJv7ogFl"
      },
      "outputs": [],
      "source": [
        "from livelossplot import PlotLosses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvBopkrnp7KE"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2113248e2034db09665ea51521aa5c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ipywidgets import Output\n",
        "OUTPUT_CONTEXT = Output()\n",
        "display(OUTPUT_CONTEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPJ4TLwjNOi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting figsize to (10, 5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "avg_epoch_loss = []\n",
        "\n",
        "plotlosses = PlotLosses(figsize=(10, 5))\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), total=num_epochs, desc='Epochs'):\n",
        "  train_loss = []\n",
        "  avg_loss = 0\n",
        "\n",
        "  for images, coordinates in (pbar:= tqdm(train_dataloader, total=len(train_dataloader), desc=f'Epoch {epoch + 1} progress')):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      images, coordinates = images.cuda(), coordinates.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "\n",
        "    loss_haversine = haversineLoss(outputs, coordinates).mean(dim=-1)\n",
        "\n",
        "    mse = mseLoss(outputs, coordinates)\n",
        "    cross_entropy = ceLoss(outputs, coordinates)\n",
        "\n",
        "    pbar.set_postfix_str(f'Loss: {loss_haversine:.5f}; \\\n",
        "                               \\nOut: {outputs.mean(dim=0).tolist()}\\\n",
        "                               \\nExpected: {coordinates.mean(dim=0).tolist()}')\n",
        "    train_loss.append(loss_haversine.detach())\n",
        "\n",
        "    with OUTPUT_CONTEXT:\n",
        "      plotlosses.update({\n",
        "          'Haversine': loss_haversine.item(),\n",
        "          'MSE': mse.item(),\n",
        "          'Cross Entropy': cross_entropy.item()\n",
        "      })\n",
        "      plotlosses.send()\n",
        "\n",
        "    loss_haversine.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  scheduler.step()\n",
        "  avg_loss = sum(train_loss) / len(train_dataloader)\n",
        "  avg_epoch_loss.append(avg_loss)\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz1NAWazkQoy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "avg_epoch_loss = torch.tensor(avg_epoch_loss, device =  'cpu')\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(range(1, len(avg_epoch_loss) + 1), avg_epoch_loss, label='Loss', color='blue', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8he5flBklMG"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq-IkvuxklYk"
      },
      "outputs": [],
      "source": [
        "avg_val_haversine_loss = 0.0\n",
        "batch_haversine = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for validation\n",
        "    for images, coordinates in tqdm(val_dataloader, total=len(val_dataloader), desc=f'Validating'):\n",
        "        # Forward pass\n",
        "        images, coordinates = images.cuda(), coordinates.cuda()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss_haversine = haversineLoss(outputs, coordinates).mean(dim=-1)\n",
        "        batch_haversine.append(loss_haversine)\n",
        "        avg_val_haversine_loss += loss_haversine\n",
        "\n",
        "# Calculate average loss and haversine\n",
        "avg_val_haversine_loss /= len(val_dataloader)\n",
        "\n",
        "print(f'Validation Haversine Loss: {avg_val_haversine_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-u02WHQk-qI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "# Back from gpu\n",
        "batch_haversine = torch.tensor(batch_haversine, device =  'cpu')\n",
        "\n",
        "# Remove outliers\n",
        "batch_haversine = median_filter(batch_haversine, size=100)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.plot(range(1, len(batch_haversine) + 1), batch_haversine, label='Validation haversine', color='green', marker='o')\n",
        "plt.xlabel('Batch Number')\n",
        "plt.ylabel('haversine')\n",
        "plt.title('Validation haversine loss per Batch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi30A-cxlMyd"
      },
      "outputs": [],
      "source": [
        "# Step 5: Visualize some predictions (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
        "\n",
        "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "\n",
        "def plot_predictions(model, dataloader, num_samples=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, coordinates in dataloader:\n",
        "\n",
        "            images = images.cuda()\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            images = images.cpu()\n",
        "            outputs = outputs.cpu()\n",
        "\n",
        "            rand_index = random.sample(range(0, len(images) - 1), min(num_samples, len(images) - 1))\n",
        "\n",
        "            for i in range(min(num_samples, len(images))):\n",
        "                i = rand_index[i]\n",
        "\n",
        "                pred_lat, pred_lon = outputs[i].cpu().numpy()\n",
        "                true_lat, true_lon = coordinates[i].numpy()\n",
        "\n",
        "                haver_err = haversineLoss(\n",
        "                                      torch.tensor(np.array([[pred_lon, pred_lat]]), dtype=torch.float32).deg2rad(),\n",
        "                                      torch.tensor(np.array([[true_lon, true_lat]]), dtype=torch.float32).deg2rad(),\n",
        "                                      )\n",
        "                # Display the image\n",
        "                img = images[i]\n",
        "                img = unnormalize(img).permute(1, 2, 0).numpy()\n",
        "                img = np.clip(img, 0, 1)\n",
        "\n",
        "                plt.imshow(img)\n",
        "                plt.title(f'Pred: ({pred_lat:.4f}, {pred_lon:.4f})\\nTrue: ({true_lat:.4f}, {true_lon:.4f})\\n Haversine: {haver_err}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # World map for better understanding of how bad our prediction is\n",
        "                geometry = [Point(pred_lon, pred_lat), Point(true_lon, true_lat)]\n",
        "                geo_df = GeoDataFrame(geometry = geometry)\n",
        "                world = gpd.read_file(url)\n",
        "                geo_df.plot(ax=world.plot(color=\"lightgrey\", figsize=(10, 6)), marker='x', c=['red', 'green'], markersize=50);\n",
        "\n",
        "                plt.show()\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODbO7jg4nJYq"
      },
      "outputs": [],
      "source": [
        "plot_predictions(model, val_dataloader, num_samples=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3-8-10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
